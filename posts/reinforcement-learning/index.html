<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Reinforcement Learning | Learning Logs</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Reinforcement Learning
Introduction
L’apprentissage par renforcement est un paradigme du machine learning dans lequel on entraîne un agent à prendre des décisions, simples ou complexes, dans un environnement, par un processus d’essais et d’erreurs.
Chaque décision prise par l’agent est ensuite soit récompensée, soit sanctionnée à l’aide d’une fonction de récompense.
L’objectif de l’agent est de maximiser la récompense cumulative au fil du temps. Ce principe s’inspire directement des mécanismes d’apprentissage observés chez les humains et les animaux."><meta name=generator content="Hugo 0.151.0"><meta name=robots content="index, follow"><link rel=stylesheet href=https://abdoulatuf-colo.github.io/abdoulatuf-learning-logs/ananke/css/main.min.efe4d852f731d5d1fbb87718387202a97aafd768cdcdaed0662bbe6982e91824.css><link rel=canonical href=https://abdoulatuf-colo.github.io/abdoulatuf-learning-logs/posts/reinforcement-learning/><meta property="og:url" content="https://abdoulatuf-colo.github.io/abdoulatuf-learning-logs/posts/reinforcement-learning/"><meta property="og:site_name" content="Learning Logs"><meta property="og:title" content="Reinforcement Learning"><meta property="og:description" content="Reinforcement Learning Introduction L’apprentissage par renforcement est un paradigme du machine learning dans lequel on entraîne un agent à prendre des décisions, simples ou complexes, dans un environnement, par un processus d’essais et d’erreurs. Chaque décision prise par l’agent est ensuite soit récompensée, soit sanctionnée à l’aide d’une fonction de récompense. L’objectif de l’agent est de maximiser la récompense cumulative au fil du temps. Ce principe s’inspire directement des mécanismes d’apprentissage observés chez les humains et les animaux."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-10-20T23:51:02+02:00"><meta property="article:modified_time" content="2025-10-20T23:51:02+02:00"><meta itemprop=name content="Reinforcement Learning"><meta itemprop=description content="Reinforcement Learning Introduction L’apprentissage par renforcement est un paradigme du machine learning dans lequel on entraîne un agent à prendre des décisions, simples ou complexes, dans un environnement, par un processus d’essais et d’erreurs. Chaque décision prise par l’agent est ensuite soit récompensée, soit sanctionnée à l’aide d’une fonction de récompense. L’objectif de l’agent est de maximiser la récompense cumulative au fil du temps. Ce principe s’inspire directement des mécanismes d’apprentissage observés chez les humains et les animaux."><meta itemprop=datePublished content="2025-10-20T23:51:02+02:00"><meta itemprop=dateModified content="2025-10-20T23:51:02+02:00"><meta itemprop=wordCount content="978"><meta name=twitter:card content="summary"><meta name=twitter:title content="Reinforcement Learning"><meta name=twitter:description content="Reinforcement Learning Introduction L’apprentissage par renforcement est un paradigme du machine learning dans lequel on entraîne un agent à prendre des décisions, simples ou complexes, dans un environnement, par un processus d’essais et d’erreurs. Chaque décision prise par l’agent est ensuite soit récompensée, soit sanctionnée à l’aide d’une fonction de récompense. L’objectif de l’agent est de maximiser la récompense cumulative au fil du temps. Ce principe s’inspire directement des mécanismes d’apprentissage observés chez les humains et les animaux."><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script></head><body class="ma0 avenir bg-near-white production"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l center items-center justify-between"><a href=https://abdoulatuf-colo.github.io/abdoulatuf-learning-logs/ class="f3 fw2 hover-white white-90 dib no-underline">Learning Logs</a><div class="flex-l items-center"><div class=ananke-socials></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l mw8 center ph3 flex-wrap justify-between"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked ttu">Posts</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">Reinforcement Learning</h1><time class="f6 mv4 dib tracked" datetime=2025-10-20T23:51:02+02:00>October 20, 2025</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h1 id=reinforcement-learning>Reinforcement Learning</h1><h2 id=introduction>Introduction</h2><p>L’apprentissage par renforcement est un paradigme du machine learning dans lequel on entraîne un agent à prendre des décisions, simples ou complexes, dans un environnement, par un processus d’essais et d’erreurs.
Chaque décision prise par l’agent est ensuite soit récompensée, soit sanctionnée à l’aide d’une fonction de récompense.
L’objectif de l’agent est de maximiser la récompense cumulative au fil du temps. Ce principe s’inspire directement des mécanismes d’apprentissage observés chez les humains et les animaux.</p><p>Prenons un exemple simple : un employé dans une entreprise. Tant qu’il fait ce qu’on attend de lui, il reçoit son salaire. S’il ne respecte pas les consignes, il risque d’être sanctionné, voire licencié. Logiquement, pour sécuriser ses revenus, il veille à accomplir correctement ses tâches.</p><p>Mais se limiter à cela ne suffit pas pour progresser : en prenant des initiatives et en allant au-delà de ses missions, il peut obtenir des promotions et augmenter significativement ses revenus. Son objectif devient alors clair : maximiser son salaire tout en restant indispensable.</p><p>Contrairement à d’autres approches de l’apprentissage automatique, l’agent n’est pas informé de manière explicite des actions à entreprendre. Il doit découvrir par lui-même quelles actions conduisent aux meilleurs résultats à travers l’expérimentation.
L’un des défis centraux de l’apprentissage par renforcement est le compromis entre exploration et exploitation :</p><ul><li><p>Exploitation : utiliser les connaissances déjà acquises pour maximiser immédiatement la récompense.</p></li><li><p>Exploration : tester de nouvelles actions afin de découvrir potentiellement de meilleures stratégies à long terme.</p></li></ul><h2 id=maximiser-le-futur-le-but-de-lagent>Maximiser le futur, le but de l&rsquo;agent</h2><p>Le retour (ou return), noté $G_t$, est la somme des récompenses futures qu’un agent s’attend à recevoir à partir de l’instant $t$</p><p>Dans le cas le plus simple, où l’épisode se termine à un instant $T$, le retour est la somme des récompenses futures :$$G_t = R_{t+1} + R_{t+2} + \dots + R_T$$</p><p>Cependant, le futur est incertain : plus on s’éloigne de l’instant $t$, plus la récompense future est soumise à de la variabilité et de l’incertitude. Pour en tenir compte, on introduit un facteur d’actualisation $\gamma$ (ou discount factor), qui permet de réduire progressivement l’importance des récompenses éloignées dans le temps :$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$</p><p>Rôle du facteur $\gamma$ :</p><ul><li>Si $\gamma = 0$ seule la récompense immédiate $R_{t+1}$ compte.</li><li>Si $\gamma \approx 1$ : L&rsquo;agent accorde une grande importance aux récompenses futures, les considérant presque aussi importantes que les récompenses immédiates.</li></ul><p>On peut également l’écrire de manière récursive :
$$G_t = R_{t+1} + \gamma G_{t+1}$$</p><p>Cette écriture est particulièrement utile pour comprendre l’équation de Bellman, qui constitue le fondement de nombreuses méthodes d’apprentissage par renforcement.</p><h3 id=perspectives-sur-le-calcul-du-retour--approche-rétrospective-vs-prospective>Perspectives sur le calcul du retour : approche rétrospective vs prospective</h3><p>Pour la fonction de retour, il est possible d’adopter deux points de vue, qui mènent à des objectifs et à des techniques différents.</p><p>Dans un premier cas, nous adoptons un point de vue rétrospectif : l’agent termine un épisode puis analyse, a posteriori, la contribution réelle de chaque étape à son résultat final. C’est le principe des méthodes Monte-Carlo, qui ne mettent à jour leurs estimations de valeur qu’une fois le retour total observé à la fin de l’épisode.</p><p>Le second point de vue est prospectif. Dans ce cas, on se trouve à un temps $t$ donné, l’agent cherche à estimer l’espérance du retour futur à partir de l’état courant $s_t$
grâce à la fonction de valeur
$𝑉(𝑠)$,ou à partir du couple état-action
$(𝑠,𝑎)$ via $𝑄(𝑠,𝑎)$. Cette perspective est au cœur des méthodes de Différence Temporelle (TD) et de la Programmation Dynamique, qui mettent à jour leurs estimations sans attendre la fin de l’épisode, en anticipant les récompenses futures.</p><p>En résumé, nous avons donc deux perspectives : l’une où l’on améliore les actions en utilisant ce qui s’est réellement passé, et l’autre où l’on estime ce qui va se passer dans le but de prendre de meilleures décisions.</p><h2 id=le-cadre-mathématique-du-rl>Le cadre mathématique du RL</h2><p>Les problèmes que l’on cherche à résoudre dans ce paradigme sont appelés Markov Decision Process (MDP).
La propriété clé d’un MDP est que la probabilité de passer d’un état à un autre, suite à une action donnée, ne dépend que de l’état courant et non de l’historique complet des états et actions précédents.
On peut représenter cela par une probabilité de transition.
$$P(S_{t+1} | S_t, A_t) = P(S_{t+1} | S_t, A_t, S_{t-1}, A_{t-1}, \dots, S_0, A_0)$$</p><h2 id=la-politiques-comment-lagent-réussit-dans-linsertitude>La politiques: Comment l&rsquo;agent réussit dans l&rsquo;insertitude</h2><p><img src=policy_representation.png alt="Representation de la politique dans le cas du frozen lake"></p><p>la fonction d&rsquo;état valeur est une fonction qqui estime l’espérance du retour à partir d&rsquo;un état si on suit une politique $\pi$.
$$v_\pi(s) = \mathbb{E}_\pi [G_t | S_t = s]$$</p><p>L&rsquo;équation de Bellman pour $v_\pi(s)$ est :
$$v_\pi(s) = \sum_{a} \pi(a|s) \sum_{s&rsquo;, r} p(s&rsquo;, r | s, a) [r + \gamma v_\pi(s&rsquo;)], \forall s \in S$$</p><p>la fonction d&rsquo;action-valeur $q_\pi(s,a)$ c’est l’espérance du retour futur si l’on prend l’action $a$ dans l’état $s$ puis continue à suivre la politique $\pi$ :
$$q_\pi(s,a) = \mathbb{E}_\pi [G_t | S_t = s, A_t = a]$$</p><p>l&rsquo;équation de Bellman pour $q_\pi(s,a)$ est :
$$q_\pi(s,a) = \sum_{s&rsquo;, r} p(s&rsquo;, r | s, a) [r + \gamma v_\pi(s&rsquo;)], \forall s \in \mathcal{S}, \forall a \in \mathcal{A}(s)$$</p><p><img src=reinforcement_learning_schema.JPG alt="Schéma de l’interaction agent–environnement. L’agent choisit une action At, reçoit une récompense Rt+1 et un nouvel éaat St+1.
Source : Sutton & Barto, Reinforcement Learning: An Introduction."></p><p>Un algorithme d’apprentissage par renforcement (Reinforcement Learning) est défini par :</p><ul><li><p>L’agent : le modèle qui interagit avec l’environnement.</p></li><li><p>Les actions : les différentes interactions possibles que l’agent peut effectuer dans l’environnement.</p></li><li><p>L’environnement : le cadre dans lequel l’agent évolue et sur lequel ses actions ont un effet.</p></li><li><p>La récompense : le signal numérique attribué à l’agent pour évaluer la qualité de son action ; elle peut être positive (récompense) ou négative (punition).</p></li><li><p>La politique (policy) : le « raisonnement » ou la stratégie de l’agent, qui détermine quelle action choisir pour un état donné de l’environnement.</p></li><li><p>La valeur (value) : une estimation de la récompense cumulée que l’agent peut espérer obtenir à long terme à partir d’un certain état.</p></li></ul><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href=https://abdoulatuf-colo.github.io/abdoulatuf-learning-logs/>&copy; Learning Logs 2025</a><div><div class=ananke-socials></div></div></div></footer></body></html>