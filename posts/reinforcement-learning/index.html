<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Reinforcement Learning | Learning Logs</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Reinforcement Learning
Introduction
Lâ€™apprentissage par renforcement est un paradigme du machine learning dans lequel on entraÃ®ne un agent Ã  prendre des dÃ©cisions, simples ou complexes, dans un environnement, par un processus dâ€™essais et dâ€™erreurs.
Chaque dÃ©cision prise par lâ€™agent est ensuite soit rÃ©compensÃ©e, soit sanctionnÃ©e Ã  lâ€™aide dâ€™une fonction de rÃ©compense.
Lâ€™objectif de lâ€™agent est de maximiser la rÃ©compense cumulative au fil du temps. Ce principe sâ€™inspire directement des mÃ©canismes dâ€™apprentissage observÃ©s chez les humains et les animaux."><meta name=generator content="Hugo 0.151.0"><meta name=robots content="index, follow"><link rel=stylesheet href=https://abdoulatuf-colo.github.io/abdoulatuf-learning-logs/ananke/css/main.min.efe4d852f731d5d1fbb87718387202a97aafd768cdcdaed0662bbe6982e91824.css><link rel=canonical href=https://abdoulatuf-colo.github.io/abdoulatuf-learning-logs/posts/reinforcement-learning/><meta property="og:url" content="https://abdoulatuf-colo.github.io/abdoulatuf-learning-logs/posts/reinforcement-learning/"><meta property="og:site_name" content="Learning Logs"><meta property="og:title" content="Reinforcement Learning"><meta property="og:description" content="Reinforcement Learning Introduction Lâ€™apprentissage par renforcement est un paradigme du machine learning dans lequel on entraÃ®ne un agent Ã  prendre des dÃ©cisions, simples ou complexes, dans un environnement, par un processus dâ€™essais et dâ€™erreurs. Chaque dÃ©cision prise par lâ€™agent est ensuite soit rÃ©compensÃ©e, soit sanctionnÃ©e Ã  lâ€™aide dâ€™une fonction de rÃ©compense. Lâ€™objectif de lâ€™agent est de maximiser la rÃ©compense cumulative au fil du temps. Ce principe sâ€™inspire directement des mÃ©canismes dâ€™apprentissage observÃ©s chez les humains et les animaux."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-10-20T23:51:02+02:00"><meta property="article:modified_time" content="2025-10-20T23:51:02+02:00"><meta itemprop=name content="Reinforcement Learning"><meta itemprop=description content="Reinforcement Learning Introduction Lâ€™apprentissage par renforcement est un paradigme du machine learning dans lequel on entraÃ®ne un agent Ã  prendre des dÃ©cisions, simples ou complexes, dans un environnement, par un processus dâ€™essais et dâ€™erreurs. Chaque dÃ©cision prise par lâ€™agent est ensuite soit rÃ©compensÃ©e, soit sanctionnÃ©e Ã  lâ€™aide dâ€™une fonction de rÃ©compense. Lâ€™objectif de lâ€™agent est de maximiser la rÃ©compense cumulative au fil du temps. Ce principe sâ€™inspire directement des mÃ©canismes dâ€™apprentissage observÃ©s chez les humains et les animaux."><meta itemprop=datePublished content="2025-10-20T23:51:02+02:00"><meta itemprop=dateModified content="2025-10-20T23:51:02+02:00"><meta itemprop=wordCount content="978"><meta name=twitter:card content="summary"><meta name=twitter:title content="Reinforcement Learning"><meta name=twitter:description content="Reinforcement Learning Introduction Lâ€™apprentissage par renforcement est un paradigme du machine learning dans lequel on entraÃ®ne un agent Ã  prendre des dÃ©cisions, simples ou complexes, dans un environnement, par un processus dâ€™essais et dâ€™erreurs. Chaque dÃ©cision prise par lâ€™agent est ensuite soit rÃ©compensÃ©e, soit sanctionnÃ©e Ã  lâ€™aide dâ€™une fonction de rÃ©compense. Lâ€™objectif de lâ€™agent est de maximiser la rÃ©compense cumulative au fil du temps. Ce principe sâ€™inspire directement des mÃ©canismes dâ€™apprentissage observÃ©s chez les humains et les animaux."><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script></head><body class="ma0 avenir bg-near-white production"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l center items-center justify-between"><a href=https://abdoulatuf-colo.github.io/abdoulatuf-learning-logs/ class="f3 fw2 hover-white white-90 dib no-underline">Learning Logs</a><div class="flex-l items-center"><div class=ananke-socials></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l mw8 center ph3 flex-wrap justify-between"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked ttu">Posts</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">Reinforcement Learning</h1><time class="f6 mv4 dib tracked" datetime=2025-10-20T23:51:02+02:00>October 20, 2025</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h1 id=reinforcement-learning>Reinforcement Learning</h1><h2 id=introduction>Introduction</h2><p>Lâ€™apprentissage par renforcement est un paradigme du machine learning dans lequel on entraÃ®ne un agent Ã  prendre des dÃ©cisions, simples ou complexes, dans un environnement, par un processus dâ€™essais et dâ€™erreurs.
Chaque dÃ©cision prise par lâ€™agent est ensuite soit rÃ©compensÃ©e, soit sanctionnÃ©e Ã  lâ€™aide dâ€™une fonction de rÃ©compense.
Lâ€™objectif de lâ€™agent est de maximiser la rÃ©compense cumulative au fil du temps. Ce principe sâ€™inspire directement des mÃ©canismes dâ€™apprentissage observÃ©s chez les humains et les animaux.</p><p>Prenons un exemple simple : un employÃ© dans une entreprise. Tant quâ€™il fait ce quâ€™on attend de lui, il reÃ§oit son salaire. Sâ€™il ne respecte pas les consignes, il risque dâ€™Ãªtre sanctionnÃ©, voire licenciÃ©. Logiquement, pour sÃ©curiser ses revenus, il veille Ã  accomplir correctement ses tÃ¢ches.</p><p>Mais se limiter Ã  cela ne suffit pas pour progresser : en prenant des initiatives et en allant au-delÃ  de ses missions, il peut obtenir des promotions et augmenter significativement ses revenus. Son objectif devient alors clair : maximiser son salaire tout en restant indispensable.</p><p>Contrairement Ã  dâ€™autres approches de lâ€™apprentissage automatique, lâ€™agent nâ€™est pas informÃ© de maniÃ¨re explicite des actions Ã  entreprendre. Il doit dÃ©couvrir par lui-mÃªme quelles actions conduisent aux meilleurs rÃ©sultats Ã  travers lâ€™expÃ©rimentation.
Lâ€™un des dÃ©fis centraux de lâ€™apprentissage par renforcement est le compromis entre exploration et exploitation :</p><ul><li><p>Exploitation : utiliser les connaissances dÃ©jÃ  acquises pour maximiser immÃ©diatement la rÃ©compense.</p></li><li><p>Exploration : tester de nouvelles actions afin de dÃ©couvrir potentiellement de meilleures stratÃ©gies Ã  long terme.</p></li></ul><h2 id=maximiser-le-futur-le-but-de-lagent>Maximiser le futur, le but de l&rsquo;agent</h2><p>Le retour (ou return), notÃ© $G_t$, est la somme des rÃ©compenses futures quâ€™un agent sâ€™attend Ã  recevoir Ã  partir de lâ€™instant $t$</p><p>Dans le cas le plus simple, oÃ¹ lâ€™Ã©pisode se termine Ã  un instant $T$, le retour est la somme des rÃ©compenses futures :$$G_t = R_{t+1} + R_{t+2} + \dots + R_T$$</p><p>Cependant, le futur est incertain : plus on sâ€™Ã©loigne de lâ€™instant $t$, plus la rÃ©compense future est soumise Ã  de la variabilitÃ© et de lâ€™incertitude. Pour en tenir compte, on introduit un facteur dâ€™actualisation $\gamma$ (ou discount factor), qui permet de rÃ©duire progressivement lâ€™importance des rÃ©compenses Ã©loignÃ©es dans le temps :$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$</p><p>RÃ´le du facteur $\gamma$ :</p><ul><li>Si $\gamma = 0$ seule la rÃ©compense immÃ©diate $R_{t+1}$ compte.</li><li>Si $\gamma \approx 1$ : L&rsquo;agent accorde une grande importance aux rÃ©compenses futures, les considÃ©rant presque aussi importantes que les rÃ©compenses immÃ©diates.</li></ul><p>On peut Ã©galement lâ€™Ã©crire de maniÃ¨re rÃ©cursive :
$$G_t = R_{t+1} + \gamma G_{t+1}$$</p><p>Cette Ã©criture est particuliÃ¨rement utile pour comprendre lâ€™Ã©quation de Bellman, qui constitue le fondement de nombreuses mÃ©thodes dâ€™apprentissage par renforcement.</p><h3 id=perspectives-sur-le-calcul-du-retour--approche-rÃ©trospective-vs-prospective>Perspectives sur le calcul du retour : approche rÃ©trospective vs prospective</h3><p>Pour la fonction de retour, il est possible dâ€™adopter deux points de vue, qui mÃ¨nent Ã  des objectifs et Ã  des techniques diffÃ©rents.</p><p>Dans un premier cas, nous adoptons un point de vue rÃ©trospectif : lâ€™agent termine un Ã©pisode puis analyse, a posteriori, la contribution rÃ©elle de chaque Ã©tape Ã  son rÃ©sultat final. Câ€™est le principe des mÃ©thodes Monte-Carlo, qui ne mettent Ã  jour leurs estimations de valeur quâ€™une fois le retour total observÃ© Ã  la fin de lâ€™Ã©pisode.</p><p>Le second point de vue est prospectif. Dans ce cas, on se trouve Ã  un temps $t$ donnÃ©, lâ€™agent cherche Ã  estimer lâ€™espÃ©rance du retour futur Ã  partir de lâ€™Ã©tat courant $s_t$
grÃ¢ce Ã  la fonction de valeur
$ğ‘‰(ğ‘ )$,ou Ã  partir du couple Ã©tat-action
$(ğ‘ ,ğ‘)$ via $ğ‘„(ğ‘ ,ğ‘)$. Cette perspective est au cÅ“ur des mÃ©thodes de DiffÃ©rence Temporelle (TD) et de la Programmation Dynamique, qui mettent Ã  jour leurs estimations sans attendre la fin de lâ€™Ã©pisode, en anticipant les rÃ©compenses futures.</p><p>En rÃ©sumÃ©, nous avons donc deux perspectives : lâ€™une oÃ¹ lâ€™on amÃ©liore les actions en utilisant ce qui sâ€™est rÃ©ellement passÃ©, et lâ€™autre oÃ¹ lâ€™on estime ce qui va se passer dans le but de prendre de meilleures dÃ©cisions.</p><h2 id=le-cadre-mathÃ©matique-du-rl>Le cadre mathÃ©matique du RL</h2><p>Les problÃ¨mes que lâ€™on cherche Ã  rÃ©soudre dans ce paradigme sont appelÃ©s Markov Decision Process (MDP).
La propriÃ©tÃ© clÃ© dâ€™un MDP est que la probabilitÃ© de passer dâ€™un Ã©tat Ã  un autre, suite Ã  une action donnÃ©e, ne dÃ©pend que de lâ€™Ã©tat courant et non de lâ€™historique complet des Ã©tats et actions prÃ©cÃ©dents.
On peut reprÃ©senter cela par une probabilitÃ© de transition.
$$P(S_{t+1} | S_t, A_t) = P(S_{t+1} | S_t, A_t, S_{t-1}, A_{t-1}, \dots, S_0, A_0)$$</p><h2 id=la-politiques-comment-lagent-rÃ©ussit-dans-linsertitude>La politiques: Comment l&rsquo;agent rÃ©ussit dans l&rsquo;insertitude</h2><p><img src=policy_representation.png alt="Representation de la politique dans le cas du frozen lake"></p><p>la fonction d&rsquo;Ã©tat valeur est une fonction qqui estime lâ€™espÃ©rance du retour Ã  partir d&rsquo;un Ã©tat si on suit une politique $\pi$.
$$v_\pi(s) = \mathbb{E}_\pi [G_t | S_t = s]$$</p><p>L&rsquo;Ã©quation de Bellman pour $v_\pi(s)$ est :
$$v_\pi(s) = \sum_{a} \pi(a|s) \sum_{s&rsquo;, r} p(s&rsquo;, r | s, a) [r + \gamma v_\pi(s&rsquo;)], \forall s \in S$$</p><p>la fonction d&rsquo;action-valeur $q_\pi(s,a)$ câ€™est lâ€™espÃ©rance du retour futur si lâ€™on prend lâ€™action $a$ dans lâ€™Ã©tat $s$ puis continue Ã  suivre la politique $\pi$ :
$$q_\pi(s,a) = \mathbb{E}_\pi [G_t | S_t = s, A_t = a]$$</p><p>l&rsquo;Ã©quation de Bellman pour $q_\pi(s,a)$ est :
$$q_\pi(s,a) = \sum_{s&rsquo;, r} p(s&rsquo;, r | s, a) [r + \gamma v_\pi(s&rsquo;)], \forall s \in \mathcal{S}, \forall a \in \mathcal{A}(s)$$</p><p><img src=reinforcement_learning_schema.JPG alt="SchÃ©ma de lâ€™interaction agentâ€“environnement. Lâ€™agent choisit une action At, reÃ§oit une rÃ©compense Rt+1 et un nouvel Ã©aat St+1.
Source : Sutton & Barto, Reinforcement Learning: An Introduction."></p><p>Un algorithme dâ€™apprentissage par renforcement (Reinforcement Learning) est dÃ©fini par :</p><ul><li><p>Lâ€™agent : le modÃ¨le qui interagit avec lâ€™environnement.</p></li><li><p>Les actions : les diffÃ©rentes interactions possibles que lâ€™agent peut effectuer dans lâ€™environnement.</p></li><li><p>Lâ€™environnement : le cadre dans lequel lâ€™agent Ã©volue et sur lequel ses actions ont un effet.</p></li><li><p>La rÃ©compense : le signal numÃ©rique attribuÃ© Ã  lâ€™agent pour Ã©valuer la qualitÃ© de son action ; elle peut Ãªtre positive (rÃ©compense) ou nÃ©gative (punition).</p></li><li><p>La politique (policy) : le Â« raisonnement Â» ou la stratÃ©gie de lâ€™agent, qui dÃ©termine quelle action choisir pour un Ã©tat donnÃ© de lâ€™environnement.</p></li><li><p>La valeur (value) : une estimation de la rÃ©compense cumulÃ©e que lâ€™agent peut espÃ©rer obtenir Ã  long terme Ã  partir dâ€™un certain Ã©tat.</p></li></ul><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href=https://abdoulatuf-colo.github.io/abdoulatuf-learning-logs/>&copy; Learning Logs 2025</a><div><div class=ananke-socials></div></div></div></footer></body></html>