+++
date = '2025-10-20T23:51:02+02:00'
draft = false
title = 'Reinforcement Learning'
+++


# Reinforcement Learning

## Introduction

Lâ€™apprentissage par renforcement est un paradigme du machine learning dans lequel on entraÃ®ne un agent Ã  prendre des dÃ©cisions, simples ou complexes, dans un environnement, par un processus dâ€™essais et dâ€™erreurs.
Chaque dÃ©cision prise par lâ€™agent est ensuite soit rÃ©compensÃ©e, soit sanctionnÃ©e Ã  lâ€™aide dâ€™une fonction de rÃ©compense.
Lâ€™objectif de lâ€™agent est de maximiser la rÃ©compense cumulative au fil du temps. Ce principe sâ€™inspire directement des mÃ©canismes dâ€™apprentissage observÃ©s chez les humains et les animaux.

Prenons un exemple simple : un employÃ© dans une entreprise. Tant quâ€™il fait ce quâ€™on attend de lui, il reÃ§oit son salaire. Sâ€™il ne respecte pas les consignes, il risque dâ€™Ãªtre sanctionnÃ©, voire licenciÃ©. Logiquement, pour sÃ©curiser ses revenus, il veille Ã  accomplir correctement ses tÃ¢ches.

Mais se limiter Ã  cela ne suffit pas pour progresser : en prenant des initiatives et en allant au-delÃ  de ses missions, il peut obtenir des promotions et augmenter significativement ses revenus. Son objectif devient alors clair : maximiser son salaire tout en restant indispensable.

Contrairement Ã  dâ€™autres approches de lâ€™apprentissage automatique, lâ€™agent nâ€™est pas informÃ© de maniÃ¨re explicite des actions Ã  entreprendre. Il doit dÃ©couvrir par lui-mÃªme quelles actions conduisent aux meilleurs rÃ©sultats Ã  travers lâ€™expÃ©rimentation.
Lâ€™un des dÃ©fis centraux de lâ€™apprentissage par renforcement est le compromis entre exploration et exploitation :

* Exploitation : utiliser les connaissances dÃ©jÃ  acquises pour maximiser immÃ©diatement la rÃ©compense.

* Exploration : tester de nouvelles actions afin de dÃ©couvrir potentiellement de meilleures stratÃ©gies Ã  long terme.

## Maximiser le futur, le but de l'agent

Le retour (ou return), notÃ© $G_t$, est la somme des rÃ©compenses futures quâ€™un agent sâ€™attend Ã  recevoir Ã  partir de lâ€™instant $t$

Dans le cas le plus simple, oÃ¹ lâ€™Ã©pisode se termine Ã  un instant $T$, le retour est la somme des rÃ©compenses futures :$$G_t = R_{t+1} + R_{t+2} + \dots + R_T$$

Cependant, le futur est incertain : plus on sâ€™Ã©loigne de lâ€™instant $t$, plus la rÃ©compense future est soumise Ã  de la variabilitÃ© et de lâ€™incertitude. Pour en tenir compte, on introduit un facteur dâ€™actualisation $\gamma$ (ou discount factor), qui permet de rÃ©duire progressivement lâ€™importance des rÃ©compenses Ã©loignÃ©es dans le temps :$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

RÃ´le du facteur $\gamma$ : 
* Si $\gamma = 0$ seule la rÃ©compense immÃ©diate $R_{t+1}$ compte.
* Si $\gamma \approx 1$ : L'agent accorde une grande importance aux rÃ©compenses futures, les considÃ©rant presque aussi importantes que les rÃ©compenses immÃ©diates.

On peut Ã©galement lâ€™Ã©crire de maniÃ¨re rÃ©cursive : 
$$G_t = R_{t+1} + \gamma G_{t+1}$$

Cette Ã©criture est particuliÃ¨rement utile pour comprendre lâ€™Ã©quation de Bellman, qui constitue le fondement de nombreuses mÃ©thodes dâ€™apprentissage par renforcement.

### Perspectives sur le calcul du retour : approche rÃ©trospective vs prospective

Pour la fonction de retour, il est possible dâ€™adopter deux points de vue, qui mÃ¨nent Ã  des objectifs et Ã  des techniques diffÃ©rents.

Dans un premier cas, nous adoptons un point de vue rÃ©trospectif : lâ€™agent termine un Ã©pisode puis analyse, a posteriori, la contribution rÃ©elle de chaque Ã©tape Ã  son rÃ©sultat final. Câ€™est le principe des mÃ©thodes Monte-Carlo, qui ne mettent Ã  jour leurs estimations de valeur quâ€™une fois le retour total observÃ© Ã  la fin de lâ€™Ã©pisode.

Le second point de vue est prospectif. Dans ce cas, on se trouve Ã  un temps $t$ donnÃ©, lâ€™agent cherche Ã  estimer lâ€™espÃ©rance du retour futur Ã  partir de lâ€™Ã©tat courant $s_t$
grÃ¢ce Ã  la fonction de valeur
$ğ‘‰(ğ‘ )$,ou Ã  partir du couple Ã©tat-action 
$(ğ‘ ,ğ‘)$ via $ğ‘„(ğ‘ ,ğ‘)$. Cette perspective est au cÅ“ur des mÃ©thodes de DiffÃ©rence Temporelle (TD) et de la Programmation Dynamique, qui mettent Ã  jour leurs estimations sans attendre la fin de lâ€™Ã©pisode, en anticipant les rÃ©compenses futures.

En rÃ©sumÃ©, nous avons donc deux perspectives : lâ€™une oÃ¹ lâ€™on amÃ©liore les actions en utilisant ce qui sâ€™est rÃ©ellement passÃ©, et lâ€™autre oÃ¹ lâ€™on estime ce qui va se passer dans le but de prendre de meilleures dÃ©cisions.


## Le cadre mathÃ©matique du RL

Les problÃ¨mes que lâ€™on cherche Ã  rÃ©soudre dans ce paradigme sont appelÃ©s Markov Decision Process (MDP).
La propriÃ©tÃ© clÃ© dâ€™un MDP est que la probabilitÃ© de passer dâ€™un Ã©tat Ã  un autre, suite Ã  une action donnÃ©e, ne dÃ©pend que de lâ€™Ã©tat courant et non de lâ€™historique complet des Ã©tats et actions prÃ©cÃ©dents.
On peut reprÃ©senter cela par une probabilitÃ© de transition. 
$$P(S_{t+1} | S_t, A_t) = P(S_{t+1} | S_t, A_t, S_{t-1}, A_{t-1}, \dots, S_0, A_0)$$

## La politiques: Comment l'agent rÃ©ussit dans l'insertitude


![Representation de la politique dans le cas du frozen lake](policy_representation.png)

la fonction d'Ã©tat valeur est une fonction qqui estime lâ€™espÃ©rance du retour Ã  partir d'un Ã©tat si on suit une politique $\pi$. 
$$v_\pi(s) = \mathbb{E}_\pi [G_t | S_t = s]$$

L'Ã©quation de Bellman pour $v_\pi(s)$ est : 
$$v_\pi(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')], \forall s \in S$$

la fonction d'action-valeur $q_\pi(s,a)$ câ€™est lâ€™espÃ©rance du retour futur si lâ€™on prend lâ€™action $a$ dans lâ€™Ã©tat $s$ puis continue Ã  suivre la politique $\pi$ : 
$$q_\pi(s,a) = \mathbb{E}_\pi [G_t | S_t = s, A_t = a]$$

l'Ã©quation de Bellman pour $q_\pi(s,a)$ est : 
$$q_\pi(s,a) = \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')], \forall s \in \mathcal{S}, \forall a \in \mathcal{A}(s)$$


![SchÃ©ma de lâ€™interaction agentâ€“environnement. Lâ€™agent choisit une action At, reÃ§oit une rÃ©compense Rt+1 et un nouvel Ã©aat St+1. 
Source : Sutton & Barto, *Reinforcement Learning: An Introduction*.](reinforcement_learning_schema.JPG)

Un algorithme dâ€™apprentissage par renforcement (Reinforcement Learning) est dÃ©fini par :

* Lâ€™agent : le modÃ¨le qui interagit avec lâ€™environnement.

* Les actions : les diffÃ©rentes interactions possibles que lâ€™agent peut effectuer dans lâ€™environnement.

* Lâ€™environnement : le cadre dans lequel lâ€™agent Ã©volue et sur lequel ses actions ont un effet.

* La rÃ©compense : le signal numÃ©rique attribuÃ© Ã  lâ€™agent pour Ã©valuer la qualitÃ© de son action ; elle peut Ãªtre positive (rÃ©compense) ou nÃ©gative (punition).

* La politique (policy) : le Â« raisonnement Â» ou la stratÃ©gie de lâ€™agent, qui dÃ©termine quelle action choisir pour un Ã©tat donnÃ© de lâ€™environnement.

* La valeur (value) : une estimation de la rÃ©compense cumulÃ©e que lâ€™agent peut espÃ©rer obtenir Ã  long terme Ã  partir dâ€™un certain Ã©tat.
